services:
  zookeeper:
    image: 'bitnami/zookeeper:latest'
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - kafka-network

  kafka-broker:
    image: 'bitnami/kafka:latest'
    container_name: kafka-broker
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_LISTENER_NAME_PREFIX=PLAINTEXT://
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka-broker:9092
    depends_on:
      - zookeeper
    networks:
      - kafka-network

  kafka-producer:
    build: ./kafka
    container_name: kafka-producer
    depends_on:
      - kafka-broker
    volumes:
      - ./input:/input
      - ./kafka:/kafka
    networks:
      - kafka-network

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_OPTS=-Dspark.deploy.defaultCores=2
    networks:
      - spark-network
    volumes:
      - ./spark:/spark

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2  
    volumes:
      - ./spark:/spark
      - ./output:/output
    depends_on:
      - spark-master
      - kafka-broker
      - namenode
    networks:
      - spark-network
      - kafka-network
      - hdfs-network

  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2  
    volumes:
      - ./spark:/spark
      - ./output:/output
    depends_on:
      - spark-master
      - kafka-broker
      - namenode
    networks:
      - spark-network
      - kafka-network
      - hdfs-network

  spark-submit:
    build: ./spark
    container_name: spark-submit-service
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_USER=hadoop
    volumes:
      - ./spark:/spark
      - ./output:/output
    depends_on:
      - spark-master
      - spark-worker
      - kafka-broker
      - namenode
    networks:
      - spark-network
      - kafka-network
      - hdfs-network

  visualization-service:
    build: ./visuals
    container_name: visualization-service
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_USER=hadoop
      - MPLCONFIGDIR=/tmp/.matplotlib
    volumes:
      - ./output:/output
      - ./visuals:/visuals
    depends_on:
      - spark-master
      - spark-worker
      - kafka-broker
      - namenode
    networks:
      - spark-network
      - kafka-network
      - hdfs-network

  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    command: hdfs namenode
    ports:
      - 9870:9870
      - 8020:8020
    env_file:
      - ./hdfs/config
    environment:
      - ENSURE_NAMENODE_DIR=/tmp/hadoop-root/dfs/name
    networks:
      - hdfs-network
    volumes:
      - ./hdfs:/hdfs

  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    command: [ "hdfs", "datanode" ]
    hostname: localhost
    ports:
      - 9864:9864
    env_file:
      - ./hdfs/config
    environment:
      - SERVICE_PRECONDITION="namenode:9870"
    networks:
      - hdfs-network

  resourcemanager:
    image: apache/hadoop:3.3.6
    container_name: resorucemanager
    hostname: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    ports:
      - 8088:8088
    env_file:
      - ./hdfs/config
    networks:
      - hdfs-network

  nodemanager:
    image: apache/hadoop:3.3.6
    container_name: nodemanager
    command: [ "yarn", "nodemanager" ]
    env_file:
      - ./hdfs/config
    networks:
      - hdfs-network

  hdfs-backup:
    build: ./backups
    container_name: hdfs-backup-service
    volumes:
      - ./backups/hdfs-backup:/backups/hdfs-backup
      - ./backups/scripts:/backup/scripts
    depends_on:
      - namenode
      - datanode
    environment:
      - HDFS_PATH=hdfs://namenode:8020/tmp/hadoop-root/dfs/data/processed_data.csv
      - LOCAL_BACKUP_DIR=/backups/hdfs-backup
    networks:
      - hdfs-network

networks:
  kafka-network:
    driver: bridge
  spark-network:
    driver: bridge
  hdfs-network:
    driver: bridge
