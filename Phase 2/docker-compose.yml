services:
  zookeeper:
    image: 'bitnami/zookeeper:latest'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - kafka-network

  kafka:
    image: 'bitnami/kafka:latest'
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_LISTENER_NAME_PREFIX=PLAINTEXT://
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper
    networks:
      - kafka-network

  producer:
    build: ./kafka
    depends_on:
      - kafka
    volumes:
      - ./input:/input
      - ./kafka:/kafka
    networks:
      - kafka-network

  spark-master:
    image: bitnami/spark:latest
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    networks:
      - spark-network
    volumes:
      - ./spark:/spark

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_EXECUTOR_MEMORY=4G
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=4
    volumes:
      - ./spark:/spark
    depends_on:
      - spark-master
    networks:
      - spark-network

  spark-submit:
    build: ./spark
    volumes:
      - ./spark:/spark
    depends_on:
      - spark-master
      - kafka
      - producer
      - namenode
    networks:
      - spark-network
      - kafka-network
      - hdfs-network
      
  # namenode:
  #     image: apache/hadoop:3
  #     hostname: namenode
  #     command: ["hdfs", "namenode"]
  #     ports:
  #       - 9870:9870
  #     env_file:
  #       - ./hdfs/hadoop.env
  #     environment:
  #         ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  #     networks:
  #      - hdfs-network

  # datanode:
  #     image: apache/hadoop:3
  #     command: ["hdfs", "datanode"]
  #     env_file:
  #       - ./hdfs/hadoop.env
  #     networks:
  #      - hdfs-network

  namenode:
      image: apache/hadoop:3
      hostname: namenode
      command: hdfs namenode && bash -c /docker-entrypoint-init.d/init-hdfs.sh
      ports:
        - 9870:9870
      env_file:
        - ./hdfs/hadoop.env
      environment:
          ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
      networks:
        - hdfs-network
      volumes:
        - ./hdfs/init-hdfs.sh:/docker-entrypoint-init.d/init-hdfs.sh

  datanode:
      image: apache/hadoop:3
      command: ["hdfs", "datanode"]
      env_file:
        - ./hdfs/hadoop.env
      networks:
        - hdfs-network

  resourcemanager:
      image: apache/hadoop:3
      hostname: resourcemanager
      command: ["yarn", "resourcemanager"]
      ports:
         - 8088:8088
      env_file:
        - ./hdfs/hadoop.env
      networks:
       - hdfs-network

  nodemanager:
      image: apache/hadoop:3
      command: ["yarn", "nodemanager"]
      env_file:
        - ./hdfs/hadoop.env
      networks:
        - hdfs-network

  # namenode:
  #   image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
  #   ports:
  #     - 9870:9870
  #     - 9000:9000
  #   volumes:
  #     - hadoop_namenode:/hadoop/dfs/name
  #     - ./hdfs/init-hdfs.sh:/docker-entrypoint-init.d/init-hdfs.sh
  #   command: ["/bin/bash", "/docker-entrypoint-init.d/init-hdfs.sh"]
  #   environment:
  #     - CLUSTER_NAME=test
  #   env_file:
  #     - ./hdfs/hadoop.env
  #   networks:
  #     - hdfs-network

  # datanode:
  #   image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
  #   volumes:
  #     - hadoop_datanode:/hadoop/dfs/data
  #   environment:
  #     - SERVICE_PRECONDITION=namenode:9870
  #   env_file:
  #     - ./hdfs/hadoop.env
  #   networks:
  #     - hdfs-network

# volumes:
#   hadoop_namenode:
#   hadoop_datanode:

networks:
  kafka-network:
    driver: bridge
  spark-network:
    driver: bridge
  hdfs-network:
    driver: bridge